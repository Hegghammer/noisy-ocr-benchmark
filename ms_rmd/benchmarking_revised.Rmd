---
title: 'OCR with Tesseract, Amazon Textract, and Google Document AI: A Benchmarking
 Experiment'
author:
  - Thomas Hegghammer^[Norwegian Defence Research Establishment (FFI) - thomas.hegghammer@ffi.no]
date: "12 September 2021"
output: 
  pdf_document:
    extra_dependencies: "subfig"
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{booktabs}
  - \usepackage{inputenc}
  - \usepackage{hyperref}
  - \usepackage{floatrow}
  - \usepackage{subfig}
  - \usepackage{graphicx}
  - \floatsetup[figure]{capposition=top}
  - \floatsetup[table]{capposition=top}
documentclass: article
biblio-style: apsr
linestretch: 2
fontsize: 12pt
indent: yes
geometry: margin=1.25in
bibliography: benchmarking.bib
thanks: I am grateful to the three anonymous reviewers and to Neil Ketchley for valuable comments. I also thank participants in the University of Oslo Political Data Science seminar on 17 June 2021 for inputs and suggestions, as well as Eddie Antonio Santos for helping solve technical questions related to the ISRI/Ocreval tool. Supplementary information and replication materials are available at https://github.com/Hegghammer/noisy-ocr-benchmark.
abstract: \noindent 
  Optical Character Recognition (OCR) can open up understudied historical documents to computational analysis, but the accuracy of OCR software varies. This article reports a benchmarking experiment comparing the performance of Tesseract, Amazon Textract, and Google Document AI on images of English and Arabic text. English-language book scans (n=322) and Arabic-language article scans (n=100) were replicated 43 times with different types of artificial noise for a corpus of 18,568 documents, generating 51,304 process requests. Document AI delivered the best results, and the server-based processors (Textract and Document AI) performed substantially better than Tesseract, especially on noisy documents. Accuracy for English was considerably higher than for Arabic. Specifying the relative performance of three leading OCR products and the differential effects of commonly found noise types can help scholars identify better OCR solutions for their research needs. The test materials have been preserved in the openly available "Noisy OCR Dataset" (NOD) for reuse in future benchmarking studies.\newline \newline \textbf{Keywords:} OCR, cloud computing, benchmarking \newline \newline \textbf{Word count:} 8,313 \newline \newline \newline \newline \newline \newline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(here)
library(stringr)
library(dplyr)
library(cowplot)
library(patchwork)
library(ggridges)
library(viridis)
library(extrafont)
library(ggplot2)
library(magrittr)
library(stats)
library(glue)
library(ggpubr)

# Load main dataset
df <- read.csv(here("data", "ocr_benchmark_master.csv"))
df$engine <- factor(df$engine, levels = c("dai", "textract", "tesseract"))
noise_types <- unique(df$noise)
df$noise <- factor(df$noise, levels = noise_types)
```

# Introduction

Few technologies hold as much promise for the social sciences and humanities as optical character recognition (OCR). Automated text extraction from digital images can open up large quantities of understudied historical documents to computational analysis, potentially generating deep new insights on the human past.

But OCR is a technology still in the making, and available software provides varying levels of accuracy. The best results are usually obtained with a tailored solution involving corpus-specific pre-processing, model training, or postprocessing, but such procedures can be labour-intensive.[^lit] Pre-trained, general OCR processors have a much higher potential for wide adoption in the scholarly community, and hence their out-of-the box performance is of scientific interest.

[^lit]: For pre-processing see, e.g, @bieniecki2007image, @dengel1997techniques, @holley2009good, @lat2018enhancing, @volk2011strategies, and @wemhoener2013creating. For model training, see, e.g., @boiangiu2016voting, @reul2018improving, @springmann2014ocr, and @wick2018comparison. For postprocessing, see, e.g., @kissos2016ocr, @strohmaier2003lexical, and @thompson2015customised.

For long, general OCR processors such as Tesseract [@tesseract2019; @patel2012optical] only delivered perfect results under what we may call laboratory conditions, i.e., on noise-free, single-column text in a clear printed font. This limited their utility for real-life historical documents, which often contain shading, blur, shine-through, stains, skewness, complex layouts, and other things that produce OCR error. Historically, general OCR processors have also struggled with non-Western languages [@kanungo1999performance], rendering them less useful for the many scholars working on documents in such languages. 

In the past decade, advances in machine learning have led to substantial improvements in standalone OCR processor performance. Moreover, the past two years have seen the arrival of server-based processors such as Amazon Textract and Google Document AI, which offer document processing via an application processing interface (API) [@walker2018web]. Media and blog coverage indicate that these processors deliver strong out-of-the-box performance[^reviews], but those tests usually involve a small number of documents. Academic benchmarking studies exist [@tafti2016ocr; @vijayarani2015performance] but the predate the server-based processors. 

[^reviews]: See, for example, Ted Han and Amanda Hickman, "Our Search for the Best OCR Tool, and What We Found", *OpenNews*, February 19, 2019 (https://source.opennews.org/articles/so-many-ocr-options/); Fabian Gringel, "Comparison of OCR tools: how to choose the best tool for your project", *Medium.com*,  January 20, 2020 (https://medium.com/dida-machine-learning/comparison-of-ocr-tools-how-to-choose-the-best-tool-for-your-project-bd21fb9dce6b);  Manoj Kukreja, "Compare Amazon Textract with Tesseract OCR â€” OCR & NLP Use Case", *TowardDataScience.com*, September 17, 2020 (https://towardsdatascience.com/compare-amazon-textract-with-tesseract-ocr-ocr-nlp-use-case-43ad7cd48748); Cem Dilmegani, "Best OCR by Text Extraction Accuracy in 2021", *AIMultiple.com*, June 6, 2021 (https://research.aimultiple.com/ocr-accuracy/). 

To find out, I conducted a benchmarking experiment comparing the performance of Tesseract, Textract, and Document AI on English and Arabic page scans. The objective was to generate statistically meaningful measurements of the accuracy of a selection of general OCR processors on document types commonly encountered in social scientific and humanities research. 

The exercise yielded specifications for the relative performance of three leading OCR products as well as the differential effects of commonly found noise types. The findings can help scholars identify better OCR solutions for their research needs. The test materials, which have been preserved in the openly available "Noisy OCR Dataset" (NOD), can be used in future research.

# Design

The experiment involved taking two document collections of 322 English-language and 100 Arabic-language page scans, replicating them 43 times with different types of artificially generated noise, processing the full corpus of ~18,500 documents in each OCR engine, and measuring the accuracy against ground truth using the Information Science Research Institute (ISRI) tool.

## Processors

I chose Tesseract, Textract, and Document AI on the basis of their wide use, reputation for accuracy, and availability for programmatic use. Budget constraints prevented the inclusion of additional reputable processors such as Adobe PDF Services and ABBYY Cloud OCR, but these can be tested in the future using the same procedure and test materials.[^cost]

[^cost]: As of September 2021, Adobe PDF Services charges a flat rate of $50 per 1,000 pages (https://www.adobe.io/apis/documentcloud/dcsdk/pdf-pricing.html, accessed 3 September 2021). ABBYY Cloud costs between $28 and $60 per 1,000 pages depending one's monthly plan and the total number of documents (see https://www.abbyy.com/cloud-ocr-sdk/licensing-and-pricing/, accessed 3 September 2021). By contrast, processing in Amazon Textract and Google Document AI costs $1.50 per 1,000 pages.

A full description of these processors is beyond the scope of this article, but Table 1 summarizes their main user-related features.[^links] All the processors are primarily designed for programmatic use and can be accessed in multiple programming languages, including R and Python. The main difference is that Tesseract is open source and installed locally, whereas Textract and Document are paid services accessed remotely via a REST API.

[^links]: For documentation, see the product websites: https://github.com/tesseract-ocr/tesseract, https://aws.amazon.com/textract/, and https://cloud.google.com/document-ai. 

```{r echo=FALSE}
#==========
# Table 1
#==========

Name <- c("Tesseract", "Textract", "Document AI")
Maintainer <- c("Tesseract OCR Project", "Amazon Web Services", "Google Cloud Services")
Installation <- c("Local", "Server-based", "Server-based")
Architecture <- c("LSTM", "Undisclosed", "Undisclosed")
Languages <- c(116, 6, "60+")
Cost <- c("Free", "$1.50 per\n1000 pages", "$1.50 per\n1000 pages")
tab <- data.frame(Name, Maintainer, Installation, Architecture, Languages, Cost)
kbl(tab, caption = "Features of Tesseract, Textract, and Document AI", booktabs = T) %>%
kable_styling(latex_options = c("striped", "scale_down"))
```

## Data

For test data, I sought materials that would be reasonably representative of those commonly studied in the social sciences and humanities. This is to say historical documents containing extended text, as opposed to forms, receipts, and other business documents, which commercial OCR engines are primarily designed for, and which tend to get the most attention in media and blog reviews.

Since many scholars work on documents in languages other than English, I also wanted to include test materials in a non-Western language. Historically, these have been less well served by OCR engines, partly because their sometimes more ornate scripts are more difficult to process than Latin script, and partly because market incentives have led the software industry to prioritize the development of English-language OCR. I chose Arabic for three reasons: its size as a world language, its alphabetic structure (which allows accuracy measurement with the ISRI tool), and the complexity of its script. Arabic is known as one of the hardest alphabetic languages for computers to process [@mariner2017optical; @jain2017unconstrained], so including it alongside English will likely provide something close to the outer performance bounds of OCR engines on alphabetic scripts. I excluded logographic scripts such as Hanzi (Chinese) and Kanji (Japanese) partly due to the difficulty of generating comparable accuracy measures and partly due to my lack of familiarity with such languages.

The English test corpus consisted of the "Old Books Dataset" [@barcha2017oldbooks], a collection of 322 colour page scans from ten books printed between 1853 and 1920 (see figures 1a and 1b and Table 2). The dataset comes as 300 DPI and 500 DPI TIFF image files accompanied by ground truth (drawn from the Project Gutenberg website) in TXT files. I used the 300 DPI files in the experiment.

The Arabic test materials were drawn from the "Yarmouk Arabic OCR Dataset" [@doush2018yarmouk], a collection of 4,587 Wikipedia articles printed out to paper and colour scanned to PDF (see figures 1c and 1d). The dataset contains ground truth in HTML and TXT files. Due to the homogeneity of the collection, a randomly selected subset of 100 pages was deemed sufficient for the experiment. 

The Yarmouk dataset is suboptimal because it does not come from historical printed documents, but it is one of very few Arabic language datasets of some size with accompanying ground truth data. The English and Arabic test materials are thus not directly analogous, and in principle the latter poses a lighter OCR challenge than the former. Another limitation of the experiment is that the test materials only includes single-column text due to the complexities involved in measuring layout parsing accuracy.  

```{r fig.cap='Sample test documents in their original state', out.width = "90%", fig.align = "center", echo=FALSE}
#============
# Figure 1
#============
include_graphics(here("images", "fig1.png"))
```

## Noise application

Social scientists and historians often deal with digitized historical documents that contain visual noise [@krishnan2012language; @ye2013document]. In practice, virtually any document that existed first on paper and were later digitized --- which is to say almost all documents produced before around 1990 and many thereafter --- is going to contain some kind of noise. Sometimes it is the original copy that is degraded; at other times the document passed through a poor photocopier, an old microfilm, or a blurry lens before reaching us. The type and degree of noise will vary across collections and individual documents, but most scholars who use archival material will encounter this problem at least occasionally.

A key objective of the experiment was therefore to gauge the effect of different types of visual noise on OCR performance. To achieve this, I programmatically applied different types of artifical noise to the test materials, so as to allow isolation of noise effects at the measurement stage. Specifically, the two dataset were duplicated 43 times, each with a different type of noise filter. The R code used for noise generation is included in the Appendix.[^doccreator]

[^doccreator]: There are other ways of generating synthetic noise, notably the powerful tool DocCreator [@journet2017doccreator]. I chose not to use DocCreator primarily because it is graphical user interface-based, and I found I could generate realistic noise more efficiently with R code.  

I began by creating a binary version of each image, so that there were two versions --- colour and greyscale --- with no added noise (see figure 2a and 2b). I then wrote functions to generate six ideal types of image noise: "blur", "weak ink", "salt and pepper", "watermark", "scribbles", and "ink stains" (see figures 2c-h). While not an exhaustive list of possible noise types, they represent several of the most common ones found in historical document scans.[^other] I applied each of the six filters to both the colour version and the binary version of the images, thus creating 12 additional versions of each image. Lastly I applied all available combinations of two noise filters to the colour and binary images, for an additional 30 versions.

[^other]: It would be possible to extend the list of noise types further, to include 10-20 different types, but this would increase the size of the corpus (and thus the processing costs) considerably, probably without affecting the broad result patterns. Since the main aim here is not to map all noise types but to compare processors, I decided on a manageable subset of noise types.

This generated a total of 44 image versions divided into three categories of noise intensity: 2 versions with no added noise, 12 versions with one layer of noise, and 30 versions with two layers of noise. This amounted to an English test corpus of 14,168 documents and an Arabic test corpus of 4,400 documents. The dataset is preserved as the "Noisy OCR Dataset" [@hegghammer2021noisy].

```{r fig.show='asis', fig.cap='Sample test document ("Old Books j020") with noise applied', out.width = "100%", fig.align = "center", echo=FALSE}
#============
# Figure 2
#============
include_graphics(here("images", "fig2a.png"))

# Fig extension separately below for layout reasons
# Windows path for fig extension: ![](C:/Users/thh/Heggcloud/Documents/Jobb/Work in progress/OCR_benchmarking/images/sample_docs_noise2.png)\
```
![](../images/fig2b.png)\

## Processing

The experiment aimed at measuring out-of-the-box performance, so documents were submitted without further preprocessing using the OCR engines' default settings.[^Arabic] While this is an uncommon use of Tesseract, it treats the engines equally and helps highlight the degree to which Tesseract is dependent on image preprocessing.

[^Arabic]: The only exception was the setting of the relevant language libraries in Tesseract.

The English corpus was submitted to all three OCR engines in a total of 42,504 document processing requests. The Arabic corpus was only submitted to Tesseract and Document AI --- since Textract does not support Arabic --- for a total of 8,800 processing requests.

The Tesseract processing was done in R with the package `tesseract` (v4.1.1). For Textract, it was carried out via the R package `paws` (v0.1.11), which provides a wrapper for the Amazon Web Services API. For Document AI, I used the R package `daiR` (v0.8.0) to access the Document AI API v1 endpoint. The processing was done in April and May of 2021 and took an estimated net total of 150-200 hours to complete. The Document AI and Textract APIs processed documents at a rate of approximately 10-15 seconds per page. Tesseract took 17 seconds per page for Arabic and 2 seconds per page for English on a Linux Desktop with a 12-core, 4.3 Ghz CPU and 64GB RAM. 

## Measurement

Accuracy was measured with the ISRI tool [@rice1996isri] in Eddie Antonio Santos's [-@santos-2019-ocr] updated version --- known as Ocreval --- which has UTF-8 support. ISRI is a simple but robust tool that has been used for OCR assessment since its creation in the mid-1990s. Alternatives exist [@alghamdi2016arabic; @carrasco2014open; @yalniz2011RETAS], but ISRI was deemed sufficient for this exercise.

ISRI compares two versions of a text --- in this case OCR output to ground truth --- and returns a range of measures for divergence, notably a document's overall character accuracy and word accuracy expressed in percent. Character accuracy is the proportion of characters in a hypothesis text that match the reference text. Any misread, misplaced, absent, or excess character is considered an error and subtracted from the numerator. This represents the so-called Levenshtein distance [@levenshtein1966binary], i.e., the minimum number of edit operations needed to correct the hypothesis text. Word accuracy is the proportion of non-stopwords in a hypothesis text that match those of the reference text.[^stopwords] 

[^stopwords]: ISRI only has an English-language stopword list (of 110 words), so in the measurements for Arabic, stopwords are included in the assessment. All else equal, this should produce slightly higher accuracy rates for Arabic, since oft-recurring words are easier for OCR engines to recognize.

Character and word accuracy are usually highly correlated, but the former punishes error harder, since each wrong character detracts from the accuracy rate.[^negative] In word accuracy, by contrast, a misspelled word counts as one error regardless of the number of wrong characters that contribute to the error. Moreover, in ISRI's implementation of word accuracy, case errors and excess words are ignored.[^wordacc]

```{r echo=FALSE}
# Code to get the exact proportions of negatives and NA values in dataset, to insert into text
neg_char <- df %>% 
  select(char_acc) %>% 
  filter(char_acc <0 | char_acc > 100) # the over hundreds are really negatives due to my regex error
neg_char_prop <- nrow(neg_char)*100/nrow(df)

na_word <- df %>%
  select(word_acc) %>% 
  filter(is.na(word_acc))
na_word_prop <- nrow(na_word)*100/nrow(df)
```

[^negative]: ISRI's character accuracy rates can actually be negative as a result of excess text. OCR engines sometimes introduce garbled text when they see images or blank areas with noise, resulting in output texts that are much longer than ground truth. Since excess characters are treated as errors and subtracted from the numerator, they can result in negative accuracy rates. In the corpus studied here, this phenomenon affected `r round(neg_char_prop, 1)` percent of the character accuracy measurements, and it occurred almost exclusively in texts processed by Tesseract.

[^wordacc]: This also means that ISRI's word accuracy tool does not yield negative rates. As Eddie Antonio Santos explains, "The wordacc algorithm creates parallel arrays of words and checks only for words present in the ground truth. It finds 'paths' from the generated file that correspond to ground truth. For this reason, it only detects as many words as there are in ground truth"; private email correspondence, 1 September 2021. However, the word accuracy tool returns NA when the hypothesis text has no recognizable words. This occurred in `r round(na_word_prop, 1)` percent of the measurements in this experiment, again almost exclusively in Tesseract output. These NAs are treated as zeroes in figures 4 to 6.

Figure 3 provides some examples of what character and word error rates may correspond to in an actual text. I will return later to the question of how error matters for analysis.

```{r fig.cap='Examples of word error effects', out.width="90%", fig.align = "center", echo=FALSE}
#============
# Figure 3
#============
include_graphics(here("images", "fig3.png"))
```

Which of the two measures is better depends on the type of document and the purpose of the analysis. For shorter texts where details matter --- such as forms and business documents --- character accuracy is considered the more relevant measure. For longer texts to be used for searches or text mining, word accuracy is commonly used as the principal metric. In the following, I therefore report word accuracy rates, transformed to word error rates by subtracting them from 100. Character accuracy rates are available in the Appendix.

# Results

The main results are shown in Figure 4 and reveal clear patterns. Document AI had consistently lower error rates, with Textract coming in a close second, and Tesseract last. More noise yielded higher error rates in all engines, but Tesseract was significantly more sensitive to noise than the two others. Overall, there was a significant performance gap between the server-based processors (Document AI and Textract) on one side and the local installation (Tesseract) on the other. Only on noise-free documents in English could Tesseract compete. 

We also see a marked performance difference across languages. Both Document AI and Tesseract delivered substantially lower accuracy for Arabic than they did for English. This was despite the Arabic corpus consisting of Internet articles in a single, very common font, while the English corpus contained old book scans in several different fonts. An analogous Arabic corpus would likely have produced an even larger performance gap. This said, Document AI represents a significant improvement on Tesseract as far as out-of-the-box Arabic OCR is concerned.

Disaggregating the data by noise type shows a more detailed picture (see Figures 5 and 6). Beyond the patterns already described, we see, for example, that both Textract and Tesseract performed somewhat better on greyscale versions of the test images than on the colour version. We also note that all engines struggled with blur, while Tesseract was much more sensitive to salt & pepper noise than the two other engines. Incidentally, it is not surprising that the ink stain filter yielded lower accuracy throughout since it completely concealed part of the text. The reason we see a bimodal distribution in the bin + blur" filters on the English corpus is that they yielded many zero values, probably as a result of the image crossing a threshold of illegibility. The same did not happen in the Arabic corpus, probably because the source images there had crisper characters at the outset. 

```{r fig.cap='Word error rates by engine and noise level for English and Arabic documents', fig.width=10,fig.height=13, echo=FALSE, message=FALSE, warning=FALSE}
#============
# Figure 4
#============

# English dataset
ob <- df %>% filter(dataset == "old_books")

ob_summary <- ob %>% 
  group_by(engine) %>% 
  summarize(means = round(mean(word_error), 1),
            peaks = max((density(word_error))$y[])) %>%
  mutate(yPos = max(peaks)*.8, xPos = c(means[1]-0.5, means[2]+0.5, means[3]))

## All engines, all noise levels

# significance
ob_dai_all <- ob %>% filter(engine=="dai") %>% select(word_error) %>% .[,1]
ob_text_all <- ob %>% filter(engine=="textract") %>% select(word_error) %>% .[,1]
ob_tess_all <- ob %>% filter(engine=="tesseract") %>% select(word_error) %>% .[,1]
pval_dai_tess <- ks.test(ob_dai_all, ob_tess_all, exact = TRUE)[[2]]
pval_dai_text <- ks.test(ob_dai_all, ob_text_all, exact = TRUE)[[2]]
pval_tess_text <- ks.test(ob_text_all, ob_tess_all, exact = TRUE)[[2]]

# figure
p1 <- ggplot(ob, aes(word_error, fill = engine)) +
  geom_density(alpha = 0.6, lwd = 0.1) +
  geom_label(data = ob_summary, 
             aes(x = xPos, y = yPos, label = means, colour=engine),
             colour="black", 
             alpha = 0.6,
             show.legend = FALSE) +
  geom_segment(data = ob_summary, 
               aes(x = means, y = 0, xend = means, yend = max(peaks)*.7, colour=engine),
               linetype = "dashed", 
               size=1, 
               alpha = 0.6, 
               show.legend = FALSE) +
  scale_color_manual(values = c("turquoise3", "yellow3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Amazon Textract", "Tesseract")) +
  scale_fill_manual(values = c("turquoise3", "yellow3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Amazon Textract", "Tesseract")) +
  theme_minimal() +
  labs(title = "ENGLISH\n\na) All documents combined (n=42,504)", x = "", y = "") +
  coord_cartesian(xlim=c(0, 50)) +
  theme(plot.title=element_text(hjust=0.5))

## All engines, no noise
ob_no_noise <- ob %>%
  filter(noise %in% noise_types[1:2])

ob_no_noise_summary <- ob_no_noise %>% 
  group_by(engine) %>% 
  summarize(means = round(mean(word_error), 1),
            peaks = max((density(word_error))$y[])) %>%
  mutate(yPos = max(peaks)*.8, xPos = c(0, 4, 8))

# significance
ob_dai_all <- ob_no_noise %>% filter(engine=="dai") %>% select(word_error) %>% .[,1]
ob_text_all <- ob_no_noise %>% filter(engine=="textract") %>% select(word_error) %>% .[,1]
ob_tess_all <- ob_no_noise %>% filter(engine=="tesseract") %>% select(word_error) %>% .[,1]
pval_dai_tess <- ks.test(ob_dai_all, ob_tess_all, exact = TRUE)[[2]]
pval_dai_text <- ks.test(ob_dai_all, ob_text_all, exact = TRUE)[[2]]
pval_tess_text <- ks.test(ob_text_all, ob_tess_all, exact = TRUE)[[2]]

# figure
p2 <- ggplot(ob_no_noise, aes(word_error, fill = engine)) +
  geom_density(alpha = 0.6, lwd = 0.1) +
  geom_label(data = ob_no_noise_summary, 
             aes(x = xPos, y = yPos, label = means, colour=engine),
             colour="black", 
             alpha = 0.6,
             show.legend = FALSE) +
  geom_segment(data = ob_no_noise_summary, 
               aes(x = means, y = 0, xend = means, yend = max(peaks)*.7, colour=engine),
               linetype = "dashed", 
               size=1, 
               alpha = 0.6, 
               show.legend = FALSE) +
  scale_color_manual(values = c("turquoise3", "yellow3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Amazon Textract", "Tesseract")) +
  scale_fill_manual(values = c("turquoise3", "yellow3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Amazon Textract", "Tesseract")) +
  theme_minimal() +
  labs(title = "b) No added noise (n=1,932)", x = "", y = "Density") +
  coord_cartesian(xlim=c(0, 50)) +
  theme(plot.title=element_text(hjust=0.5))

## All engines, one noise level

ob_one_layer <- ob %>%
  filter(noise %in% noise_types[3:14])

ob_one_layer_summary <- ob_one_layer %>% 
  group_by(engine) %>% 
  summarize(means = round(mean(word_error), 1),
            peaks = max((density(word_error))$y[])) %>%
  mutate(yPos = max(peaks)*.8, xPos = means)

# significance
ob_dai_all <- ob_one_layer %>% filter(engine=="dai") %>% select(word_error) %>% .[,1]
ob_text_all <- ob_one_layer %>% filter(engine=="textract") %>% select(word_error) %>% .[,1]
ob_tess_all <- ob_one_layer %>% filter(engine=="tesseract") %>% select(word_error) %>% .[,1]
pval_dai_tess <- ks.test(ob_dai_all, ob_tess_all, exact = TRUE)[[2]]
pval_dai_text <- ks.test(ob_dai_all, ob_text_all, exact = TRUE)[[2]]
pval_tess_text <- ks.test(ob_text_all, ob_tess_all, exact = TRUE)[[2]]

# figure
p3 <- ggplot(ob_one_layer, aes(word_error, fill = engine)) +
  geom_density(alpha = 0.6, lwd = 0.1) +
  geom_label(data = ob_one_layer_summary, 
             aes(x = xPos, y = yPos, label = xPos, colour=engine),
             colour="black", 
             alpha = 0.6,
             show.legend = FALSE) +
  geom_segment(data = ob_one_layer_summary, 
               aes(x = means, y = 0, xend = means, yend = max(peaks)*.7, colour=engine),
               linetype = "dashed", 
               size=1, 
               alpha = 0.6, 
               show.legend = FALSE) +
  scale_color_manual(values = c("turquoise3", "yellow3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Amazon Textract", "Tesseract")) +
  scale_fill_manual(values = c("turquoise3", "yellow3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Amazon Textract", "Tesseract")) +
  theme_minimal() +
  labs(title = "c) One noise layer (n=11,592)", x = "", y = "") +
  coord_cartesian(xlim=c(0, 50)) +
  theme(plot.title=element_text(hjust=0.5))


## All engines, two noise levels
ob_two_layers <- ob %>%
  filter(noise %in% noise_types[15:44])

ob_two_layers_summary <- ob_two_layers %>% 
  group_by(engine) %>% 
  summarize(means = round(mean(word_error), 1),
            peaks = max((density(word_error))$y[])) %>%
  mutate(yPos = max(peaks)*.8, xPos = means)

# significance
ob_dai_all <- ob_two_layers %>% filter(engine=="dai") %>% select(word_error) %>% .[,1]
ob_text_all <- ob_two_layers %>% filter(engine=="textract") %>% select(word_error) %>% .[,1]
ob_tess_all <- ob_two_layers %>% filter(engine=="tesseract") %>% select(word_error) %>% .[,1]
pval_dai_tess <- ks.test(ob_dai_all, ob_tess_all, exact = TRUE)[[2]]
pval_dai_text <- ks.test(ob_dai_all, ob_text_all, exact = TRUE)[[2]]
pval_tess_text <- ks.test(ob_text_all, ob_tess_all, exact = TRUE)[[2]]

# figure
p4 <- ggplot(ob_two_layers, aes(word_error, fill = engine)) +
  geom_density(alpha = 0.6, lwd = 0.1) +
  geom_label(data = ob_two_layers_summary, 
             aes(x = xPos, y = yPos, label = xPos, colour=engine),
             colour="black", 
             alpha = 0.6,
             show.legend = FALSE) +
  geom_segment(data = ob_two_layers_summary, 
               aes(x = means, y = 0, xend = means, yend = max(peaks)*.7, colour=engine),
               linetype = "dashed", 
               size=1, 
               alpha = 0.6, 
               show.legend = FALSE) +
  scale_color_manual(values = c("turquoise3", "yellow3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Amazon Textract", "Tesseract")) +
  scale_fill_manual(values = c("turquoise3", "yellow3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Amazon Textract", "Tesseract")) +
  theme_minimal() +
  labs(title = "d) Two noise layers (n=28,980)", x = "Word error (percent)", y = "") +
  coord_cartesian(xlim=c(0, 50)) +
  theme(plot.title=element_text(hjust=0.5))

###############
# Yarmouk

yar <- df %>%  filter(dataset == "yarmouk")

yar_summary <- yar %>% 
  group_by(engine) %>% 
  summarize(means = round(mean(word_error), 1),
            peaks = max((density(word_error))$y[])) %>%
  mutate(yPos = max(peaks)*.8, xPos = means)

## All engines, all noise levels - see 1) above

# significance
yar_dai_all <- yar %>% filter(engine=="dai") %>% select(word_error) %>% .[,1]
yar_tess_all <- yar %>% filter(engine=="tesseract") %>% select(word_error) %>% .[,1]
pval_dai_tess <- ks.test(yar_dai_all, yar_tess_all, exact = TRUE)[[2]]

# figure
p5 <- ggplot(yar, aes(word_error, fill = engine)) +
  geom_density(alpha = 0.6, lwd = 0.1, show.legend = FALSE) +
  geom_label(data = yar_summary, 
             aes(x = xPos, y = yPos, label = xPos, colour=engine),
             colour="black", 
             alpha = 0.6,
             show.legend = FALSE) +
  geom_segment(data = yar_summary, 
               aes(x = means, y = 0, xend = means, yend = max(peaks)*.7, colour=engine),
               linetype = "dashed", 
               size=1, 
               alpha = 0.6, 
               show.legend = FALSE) +
  scale_color_manual(values = c("turquoise3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Tesseract")) +
  scale_fill_manual(values = c("turquoise3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Tesseract")) +
  theme_minimal() +
  labs(title = "ARABIC\n\ne) All documents combined (n=8,800)", x = "", y = "") +
  coord_cartesian(xlim=c(0, 50)) +
  theme(plot.title=element_text(hjust=0.5))

## All engines, no noise

yar_no_noise <- yar %>%
  filter(noise %in% noise_types[1:2])

yar_no_noise_summary <- yar_no_noise %>% 
  group_by(engine) %>% 
  summarize(means = round(mean(word_error), 1),
            peaks = max((density(word_error))$y[])) %>%
  mutate(yPos = max(peaks)*.8, xPos = means)

# significance
yar_dai_all <- yar_no_noise %>% filter(engine=="dai") %>% select(word_error) %>% .[,1]
yar_tess_all <- yar_no_noise %>% filter(engine=="tesseract") %>% select(word_error) %>% .[,1]
pval_dai_tess <- ks.test(yar_dai_all, yar_tess_all, exact = TRUE)[[2]]

# figure
p6 <- ggplot(yar_no_noise, aes(word_error, fill = engine)) +
  geom_density(alpha = 0.6, lwd = 0.1, show.legend = FALSE) +
  geom_label(data = yar_no_noise_summary, 
             aes(x = xPos, y = yPos, label = xPos, colour=engine),
             colour="black", 
             alpha = 0.6,
             show.legend = FALSE) +
  geom_segment(data = yar_no_noise_summary, 
               aes(x = means, y = 0, xend = means, yend = max(peaks)*.7, colour=engine),
               linetype = "dashed", 
               size=1, 
               alpha = 0.6, 
               show.legend = FALSE) +
  scale_color_manual(values = c("turquoise3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Tesseract")) +
  scale_fill_manual(values = c("turquoise3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Tesseract")) +
  theme_minimal() +
  labs(title = "f) No added noise (n=400)", x = "", y = "") +
  coord_cartesian(xlim=c(0, 50)) +
  theme(plot.title=element_text(hjust=0.5))

## All engines, one noise level
yar_one_layer <- yar %>%
  filter(noise %in% noise_types[3:14])

yar_one_layer_summary <- yar_one_layer %>% 
  group_by(engine) %>% 
  summarize(means = round(mean(word_error), 1),
            peaks = max((density(word_error))$y[])) %>%
  mutate(yPos = max(peaks)*.8, xPos = means)

# significance
yar_dai_all <- yar_one_layer %>% filter(engine=="dai") %>% select(word_error) %>% .[,1]
yar_tess_all <- yar_one_layer %>% filter(engine=="tesseract") %>% select(word_error) %>% .[,1]
pval_dai_tess <- ks.test(yar_dai_all, yar_tess_all, exact = TRUE)[[2]]

# figure
p7 <- ggplot(yar_one_layer, aes(word_error, fill = engine)) +
  geom_density(alpha = 0.6, lwd = 0.1, show.legend = FALSE) +
  geom_label(data = yar_one_layer_summary, 
             aes(x = xPos, y = yPos, label = xPos, colour=engine),
             colour="black", 
             alpha = 0.6,
             show.legend = FALSE) +
  geom_segment(data = yar_one_layer_summary, 
               aes(x = means, y = 0, xend = means, yend = max(peaks)*.7, colour=engine),
               linetype = "dashed", 
               size=1, 
               alpha = 0.6, 
               show.legend = FALSE) +
  scale_color_manual(values = c("turquoise3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Tesseract")) +
  scale_fill_manual(values = c("turquoise3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Tesseract")) +
  theme_minimal() +
  labs(title = "g) One noise layer (n=2,400)", x = "", y = "") +
  coord_cartesian(xlim=c(0, 50)) +
  theme(plot.title=element_text(hjust=0.5))

## All engines, two noise levels

yar_two_layers <- yar %>%
  filter(noise %in% noise_types[15:44])

yar_two_layers_summary <- yar_two_layers %>% 
  group_by(engine) %>% 
  summarize(means = round(mean(word_error), 1),
            peaks = max((density(word_error))$y[])) %>%
  mutate(yPos = max(peaks)*.8, xPos = means)

# significance
yar_dai_all <- yar_two_layers %>% filter(engine=="dai") %>% select(word_error) %>% .[,1]
yar_tess_all <- yar_two_layers %>% filter(engine=="tesseract") %>% select(word_error) %>% .[,1]
pval_dai_tess <- ks.test(yar_dai_all, yar_tess_all, exact = TRUE)[[2]]

# figure
p8 <- ggplot(yar_two_layers, aes(word_error, fill = engine)) +
  geom_density(alpha = 0.6, lwd = 0.1, show.legend = FALSE) +
  geom_label(data = yar_two_layers_summary, 
             aes(x = xPos, y = yPos, label = xPos, colour=engine),
             colour="black", 
             alpha = 0.6,
             show.legend = FALSE) +
  geom_segment(data = yar_two_layers_summary, 
               aes(x = means, y = 0, xend = means, yend = max(peaks)*.7, colour=engine),
               linetype = "dashed", 
               size=1, 
               alpha = 0.6, 
               show.legend = FALSE) +
  scale_color_manual(values = c("turquoise3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Tesseract")) +
  scale_fill_manual(values = c("turquoise3", "red2"),
                    name = "",
                    labels = c("Google Document AI", "Tesseract")) +
  theme_minimal() +
  labs(title = "h) Two noise layers (n=6000)", x = "", y = "") +
  coord_cartesian(xlim=c(0, 50)) +
  theme(plot.title=element_text(hjust=0.5))

patchwork <- (p1 | p5) / (p2| p6) / (p3 | p7) / (p4 | p8) +  plot_layout(guides = "collect") & theme(legend.position = 'bottom')

patchwork  + plot_annotation(title="\nMean error rates in coloured boxes. p < .05 in Kolmogorov-Smirnov tests for all distribution pairs. X axes cropped.\n")
```

```{r fig.cap='Word error rates by engine and noise type for English-language documents', fig.width=10,fig.height=13, echo=FALSE, message=FALSE, warning=FALSE}
#============
# Figure 5
#============

ob_tess <- ob %>% filter(engine == "tesseract")
ob_tex <- ob %>% filter(engine == "textract")
ob_dai <- ob %>% filter(engine == "dai")

ob_tess_summary <- ob_tess %>% 
  group_by(noise) %>% 
  summarize(word_error_mean = round(mean(word_error), 1))

ob_tex_summary <- ob_tex %>% 
  group_by(noise) %>% 
  summarize(word_error_mean = round(mean(word_error), 1))

ob_dai_summary <- ob_dai %>% 
  group_by(noise) %>% 
  summarize(word_error_mean = round(mean(word_error), 1))

p9 <- ggplot(ob_dai, aes(x = word_error, y = rev(noise))) +
  geom_density_ridges(aes(fill = engine), 
                      scale = 1.5, 
                      size = 0.2, 
                      fill = "#00AFBB", 
                      quantile_lines = TRUE, 
                      quantiles = 2, 
                      vline_size = 1.1, 
                      vline_color = "green") +
  geom_text(data = ob_dai_summary, 
            aes(y = noise, x = 55, label = rev(word_error_mean)),
            nudge_y = 0.5,
            fontface = "bold",
            size = 3) +
  annotate("text", x = 55, y = 45.2, label = "mean", fontface = "bold", size = 3) +
  annotate("text", x = 2, y = 46.5, label = "\nmedian", fontface = "bold", size = 3, color = "green2") +
  scale_x_continuous(limits = c(0,100)) +
  labs(x = "", y="") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_discrete(labels = rev(unique(ob_dai$noise))) +
  ggtitle("Google Document AI") +
  theme(text = element_text(face = "bold"),
        plot.title=element_text(hjust=0.5))

p10 <- ggplot(ob_tex, aes(x = word_error,
                          y = rev(noise))) +
  geom_density_ridges(aes(fill = engine), 
                      scale = 1.5, 
                      size = 0.2, 
                      fill = "#E7B800", 
                      quantile_lines = TRUE, 
                      quantiles = 2, 
                      vline_size = 1.1, 
                      vline_color = "green") +
  geom_text(data = ob_tex_summary, 
            aes(y = noise, x = 55, label = rev(word_error_mean)),
            nudge_y = 0.5,
            fontface = "bold",
            size = 3) +
  annotate("text", x = 55, y = 45.2, label = "", fontface = "bold", size = 3) +
  annotate("text", x = 5, y = 46.5, label = "\n", fontface = "bold", size = 3) +
  scale_x_continuous(limits = c(0,100)) +
  labs(x = "", y="") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_discrete(labels = rev(unique(ob_dai$noise))) +
  theme(axis.text.y=element_blank()) +
  ggtitle("Amazon Textract") +
  theme(text = element_text(face = "bold"),
        plot.title=element_text(hjust=0.5))

p11 <- ggplot(ob_tess, aes(x = word_error,
                           y = rev(noise))) +
  geom_density_ridges(aes(fill = engine), 
                      scale = 1.5, 
                      size = 0.2, 
                      fill = "#FC4E07", 
                      quantile_lines = TRUE, 
                      quantiles = 2, 
                      vline_size = 1.1, 
                      vline_color = "green") +
  geom_text(data = ob_tess_summary,
            aes(y = noise, x = 55, label = rev(word_error_mean)),
            nudge_y = 0.5,
            fontface = "bold",
            size = 3) +
  annotate("text", x = 55, y = 45.2, label = "", fontface = "bold", size = 3) +
  annotate("text", x = 5, y = 46.5, label = "\n", fontface = "bold", size = 3) +
  scale_x_continuous(limits = c(0,100)) +
  labs(x = "", y="") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_discrete(labels = rev(unique(ob_dai$noise))) +
  theme(axis.text.y=element_blank()) +
  ggtitle("Tesseract") +
  theme(text = element_text(face = "bold"),
        plot.title=element_text(hjust=0.5))

patch <- p9 + p10 + p11

patch + plot_annotation(subtitle = "\nData: Single-column text in historical book scans with noise added articifially (n=42,504; 322 per engine and noise type).\nNoise codes: 'col'=colour, 'bin'=binary, 'blur'=blur, 'weak'=weak ink, 'snp'=salt&pepper, 'wm'=watermark, 'scrib'=scribbles, 'ink'=ink stains.\n",
                        caption="Word error (percent)",
                        theme=theme(plot.subtitle = element_text(size=11),
                                    plot.caption=element_text(hjust=0.5, size=12,
                                                              margin=margin(t=0))))
```

```{r echo=FALSE, fig.cap='Word error rates by engine and noise type for Arabic-language documents', fig.height=13, fig.width=10, message=FALSE, warning=FALSE}
# https://community.rstudio.com/t/ggridges-add-labels-showing-median-values/8767/3

#============
# Figure 6
#============

yar_tess <- yar %>% filter(engine == "tesseract")
yar_dai <- yar %>% filter(engine == "dai")
yar_dai_summary <- yar_dai %>% 
  group_by(noise) %>% 
  summarize(word_error_mean = round(mean(word_error), 1))
yar_tess_summary <- yar_tess %>% 
  group_by(noise) %>% 
  summarize(word_error_mean = round(mean(word_error), 1))

p12 <- ggplot(yar_dai, aes(x = word_error, 
                           y = rev(noise))) +
  geom_density_ridges(aes(fill = engine), 
                      scale = 1.5, 
                      size = 0.2, 
                      fill = "#00AFBB", 
                      quantile_lines = TRUE, 
                      quantiles = 2, 
                      vline_size = 1.1, 
                      vline_color = "green") +
  geom_text(data = yar_dai_summary, 
            aes(y = noise, x = 55, label = rev(word_error_mean)), nudge_y = 0.5,
            fontface = "bold",
            size = 3) +
  annotate("text", x = 55, y = 45.2, label = "mean", fontface = "bold", size = 3) +
  annotate("text", x = 7, y = 46, label = "\nmedian", fontface = "bold", size = 3, color = "green4") +
  scale_x_continuous(limits = c(0,100)) +
  labs(x = "", y="") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_discrete(labels = rev(unique(yar_dai$noise))) +
  ggtitle("Google Document AI") +
  theme(text = element_text(face = "bold"),
        plot.title=element_text(hjust=0.5))

p13 <- ggplot(yar_tess, aes(x = word_error,
                            y = rev(noise))) +
  geom_density_ridges(aes(fill = engine), 
                      scale = 1.5, 
                      size = 0.2, 
                      fill = "#FC4E07", 
                      quantile_lines = TRUE, 
                      quantiles = 2, 
                      vline_size = 1.1, 
                      vline_color = "green") +
  geom_text(data = yar_tess_summary, 
             aes(y = noise, 
                 x = 55,
                 #x = rev(word_error_mean), 
                 label = rev(word_error_mean)),
            nudge_y = 0.5,
            fontface = "bold",
            size = 3) +
  annotate("text", x = 55, y = 45.2, label = "") +
  annotate("text", x = 5, y = 46, label = "\n") +
  scale_x_continuous(limits = c(0,100)) +
  labs(x = "", y="") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_y_discrete(labels = rev(unique(yar_dai$noise))) +
  theme(axis.text.y=element_blank()) +
  ggtitle("Tesseract") +
  theme(text = element_text(face = "bold"),
        plot.title=element_text(hjust=0.5))

patch <- p12 + plot_spacer() + p13 

patch + plot_annotation(subtitle = "Data: Single-column text in image scans of Arabic Wikipedia pages with noise added articifially (n = 8800; 100 per engine and noise type).\nNoise codes: 'col'=colour, 'bin'=binary, 'blur'=blur, 'weak'=weak ink, 'snp'=salt&pepper, 'wm'=watermark, 'scrib'=scribbles, 'ink'=ink stains.",
                        caption="Word error (percent)",
                        theme=theme(plot.subtitle=element_text(size=11),
                                    plot.caption=element_text(hjust=0.5, 
                                                              size=12, 
                                                              margin=margin(t=0))))
```

\newpage

# Implications

When is it worth paying for better OCR accuracy? The answer depends on a range of situational factors, such as the state of the corpus, the utility function of the researcher, and the intended use case. 

Much hinges on the corpus itself. As we have seen, accuracy gains increase with noise and are higher for certain types of noise. Moreover, if the corpus contains many different types of noise, a better general processor will save the researcher relatively more preprocessing time. Unfortunately we lack good tools for (ground truth-free) noise diagnostics, but there are ways to obtain some information about the noise state of the corpus [@gupta2015automatic; @lins2010automatically; @reffle2013unsupervised]. Finally, the size of the dataset matters, since processing costs scale with the number of documents while accuracy gains do not.

The calculus also depends on the economic situation of the researcher. Aside from absolute size of one's budget, a key consideration is labour cost, since cloud-based processing is in some sense a substitute for Tesseract processing with additional labour input. The latter option will thus make more sense for a student than for a professor and more sense for the faster programmer.

Last but not least is the intended use of the OCRed text. If the aim is to recreate a perfect plaintext copy of the original document for, say, a browseable digital archive, then every percentage point matters. But if the purpose is to build a topic model or conduct a sentiment analysis, it is not obvious that a cleaner text will always yield better end results. The downstream effects of OCR error is a complex topic that cannot be explored in full here, but we can get some pointers by looking at the available literature and doing some tests of our own. 

Existing research suggests that the effects of OCR error vary by analytical toolset. Broadly speaking, topic models have proved relatively robust to OCR inaccuracy [@colavizza2021goodenough; @grant2021topic; @mutuvi2018evaluating; @su2015topic], with Van Strien et al [-@vanstrien2020assessing] suggesting a baseline for acceptable OCR accuracy as low as 80 percent. Classification models have been somewhat more error-sensitive, although the results here have been mixed [@colavizza2021goodenough; @murata2006impact; @stein2006effect; @vanstrien2020assessing]. The biggest problems seem to arise in natural language processing (NLP) tasks where details matter, such as part-of-speech tagging and named entity recognition [@hamdi2019analysis; @lopresti2009optical; @miller2000named; @vanstrien2020assessing].

To illustrate some of these dynamics and add to the empirical knowledge of OCR error effects, we can run some simple tests on the English-language materials from our benchmarking exercise. The Old Books dataset is small, but similar in kind to the types of text collections studied by historians and social scientists, and hence a reasonably representative test corpus. In the following, I look at OCR error in four analytical settings: sentiment analysis, classification, topic modelling, and named entity recognition. I exploit the fact that the benchmarking exercise yielded 132 different variants (3 engines and 44 noise types) of the Old Book corpus, each with a somewhat different amount of OCR error.[^clarify] By running the same analyses on all text variants, we should get a sense of how OCR error can affect substantive findings. This said, the exercise as a whole is a back-of-the-envelope test insofar as it covers only a small subset of available text mining methods and does not implement any of them as fully as one would in a real-life setting.
    
[^clarify]: In all of the below, "OCR error" refers to word error rates computed with the ISRI tool.
    
## Sentiment analysis

Faced with a corpus like Old Books (see Table 2), a researcher might want to explore text sentiment, for example to examine differences between authors or over time. Using the R package `quanteda`s LSD 2015 and ANEW dictionaries, I generated document-level sentiment polarity and valence scores for all variants of the corpus after standard preprocessing. To assess the effect of OCR error, I calculated the absolute difference between these scores and those of the ground truth version of the corpus. Figures 7a-d indicate that these differences increase only slightly with OCR error, but also that, for sentiment polarity, the variance is such that just a few percent OCR error can produce sentiment scores that diverge from ground truth scores by up to two whole points at the document level.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#============
# Table 2
#============

book_details <- read.csv(here("data", "book_details.csv"))
book_details$pages <- paste0(book_details$pages_present, " (", book_details$pages_total, ")")
tbl <- book_details %>% select(titles, authors, print_year, pages, token_count) %>% arrange(print_year)
kbl(tbl, 
    col.names = c("Title", "Author", "Year", "Pages", "Words"),
    caption = "Composition of Old Books corpus", 
    booktabs = T,
    linesep = "") %>%
  kable_styling(latex_options = c("striped", "scale_down")) 
```

```{r echo=FALSE}
#=====================
# aesthetics variables
#=====================

points_color = "black"
trendline_color = "pink"
gt_color = "green3"
r2_color = "red"
```

```{r echo=FALSE, fig.cap='OCR error and sentiment analysis accuracy', fig.height=10, fig.width=10, message=FALSE, warning=FALSE}
#============
# Figure 7
#============

# load data
sent <- read.csv(here("data", "sentiment_master.csv"))
sent_nogt <- sent %>% filter(!noise=="gt")
sent_nogt$abs_diff <- abs(sent_nogt$diff)
sent_nogt$pleasure_abs_diff <- abs(sent_nogt$pleasure_diff)
sent_nogt$arousal_abs_diff <- abs(sent_nogt$arousal_diff)
sent_nogt$dominance_abs_diff <- abs(sent_nogt$dominance_diff)

# A) Polarity 
p14 <- ggplot(sent_nogt, aes(x = word_error, y = abs_diff)) +
  geom_point(color = points_color, alpha = 0.2) +
  geom_smooth(method = "lm", color = "pink", fill = "pink") +
  stat_regline_equation(label.x = 30, label.y = 1, aes(label = ..eq.label..),  geom = "label", color = r2_color, fill = "white", size = 4, fontface = "bold") +
  scale_x_continuous(limits = c(0,75),
                     breaks = c(0, 20, 40, 60)) +
  scale_y_continuous(limits = c(0,4)) +
  labs(title = "a) Polarity") +
  theme_minimal() +
  theme(text = element_text(face = "bold"),
        plot.title = element_text(face = "bold", hjust = 0.5),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12)
        )

# B) Pleasure
p15 <- ggplot(sent_nogt, aes(x = word_error, y = pleasure_abs_diff)) +
  geom_point(color = points_color, alpha = 0.2) +
  geom_smooth(method = "lm", color = "pink", fill = "pink") +
  stat_regline_equation(label.x = 30, label.y = 1, aes(label = ..eq.label..),  geom = "label", color = r2_color, fill = "white", size = 4, fontface = "bold") +
  scale_x_continuous(limits = c(0,75),
                     breaks = c(0, 20, 40, 60)) +
  scale_y_continuous(limits = c(0,4)) +
  labs(title = "b) Valence: pleasure") +
  theme_minimal() +
  theme(text = element_text(face = "bold"),
        plot.title = element_text(face = "bold", hjust = 0.5),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12)
        )

# C) Arousal
p16 <- ggplot(sent_nogt, aes(x = word_error, y = arousal_abs_diff)) +
  geom_point(color = points_color, alpha = 0.2) +
  geom_smooth(method = "lm", color = "pink", fill = "pink") +
  stat_regline_equation(label.x = 30, label.y = 1, aes(label = ..eq.label..),  geom = "label", color = r2_color, fill = "white", size = 4, fontface = "bold") +
  scale_x_continuous(limits = c(0,75),
                     breaks = c(0, 20, 40, 60)) +
  scale_y_continuous(limits = c(0,4)) +
  labs(title = "c) Valence: arousal",
       y = "Points difference") +
  theme_minimal() +
  theme(text = element_text(face = "bold"),
        plot.title = element_text(face="bold", hjust=0.5),
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12)
        )

# D) Dominance
p17 <- ggplot(sent_nogt, aes(x = word_error, y = dominance_abs_diff)) +
  geom_point(color = points_color, alpha = 0.2) +
  geom_smooth(method = "lm", color = "pink", fill = "pink") +
  stat_regline_equation(label.x = 30, label.y = 1, aes(label = ..eq.label..),  geom = "label", color = r2_color, fill = "white", size = 4, fontface = "bold") +
  scale_x_continuous(limits = c(0,75),
                     breaks = c(0, 20, 40, 60)) +
  scale_y_continuous(limits = c(0,4)) +
  labs(title = "d) Valence: dominance") +
  theme_minimal() +
  theme(text = element_text(face = "bold"),
        plot.title=element_text(face = "bold", hjust = 0.5),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12)
        )

# Collate with Patchwork
patch <- (p14 | p15) / (p16 | p17)
patch + plot_annotation(subtitle = "\nDifferences in document-level (n=42504) sentiment polarity and valence between OCR processed versions of 'Old Books' and\nground truth. Scores calculated with Quanteda's 'LSD 2015' and 'ANEW' dictionaries. Y axis is absolute difference in\npolarity/valence points from ground truth score. X axis cropped.",
                        caption = "\nWord error (percent)",
                        theme = theme(plot.subtitle = element_text(size = 12),
                                      plot.caption = element_text(hjust = 0.5, size = 12, margin = margin(t = 0), face = "bold")))
```

## Text classification 

Another common analytical task is text classification. Imagine that we knew which works were represented in the Old Books corpus, but not which work each document belonged to. We could then handcode a subset and train an algorithm to classify the rest. Since we happen to have pre-coded metadata we can easily simulate this exercise. I trained two multiclass classifiers --- Random Forest and Support-Vector Machine --- to retrieve the book from which a document was drawn. To avoid imbalance, I removed the smallest subset ("Engraving of Lions, Tigers, Panthers, Leopards, Dogs, &C.") and was left with 9 classes and 314 documents. For each variant of the corpus I preprocessed the texts, split them 70/30 for training and testing, and fit the models using the `tidymodels` R package. Figures 8a and 8b show the results. We see that OCR error has only a small negative effect on classifier accuracy up to a threshold of around 20 percent OCR error, after which accuracy plummets.

```{r echo=FALSE, fig.cap='OCR error and multiclass classifier accuracy', fig.height=6, fig.width=10, message=FALSE, warning=FALSE}
#============
# Figure 8
#============

# Load data
clf <- read.csv(here("data", "classifiers_and_ner_master.csv"))

# A) SVM
gt_intercept <- clf %>% select(svm_accuracy) %>% .[1,]
highlight_df <- clf %>% filter(noise == "gt")

p18 <- ggplot(clf, aes(x = mean_word_error, y = svm_accuracy, )) +
  geom_point(color = points_color, size = 4, alpha = .5) +
  geom_point(data = highlight_df, color = gt_color, size = 4) +
  geom_smooth(method = "lm", color = "pink", fill = "pink") +
  stat_regline_equation(label.x = 40, label.y = .55, aes(label = ..eq.label..), geom = "label", color = r2_color, size = 4, fontface="bold") +
  geom_hline(yintercept=gt_intercept, linetype="dashed", color = gt_color, size = .8) +
  annotate("text", x = 10, y = .9, label = "Ground truth", color = gt_color, fontface="bold", size = 4) +
  scale_x_continuous(limits = c(0,75),
                     breaks = c(0, 20, 40, 60)) +
  scale_y_continuous(limits = c(0,1),
                     breaks = c(0, .2, .4, .6, .8, 1)) +
  labs(title = "a) Support-vector machine\n",
       y = "Classifier accuracy") +
  theme_minimal() +
  theme(text = element_text(face = "bold"),
        plot.title = element_text(face = "bold"),
        axis.title.x = element_blank(),
        axis.title.y = element_text(size = 11),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12)
        )

# B) Random forest
gt_intercept <- clf %>% select(rf_accuracy) %>% .[1,]

p19 <- ggplot(clf, aes(x = mean_word_error, y = rf_accuracy)) +
  geom_point(color = points_color, size = 4, alpha = .5) +
  geom_point(data = highlight_df, color = gt_color, size = 4) +
  geom_smooth(method = "lm", color = "pink", fill = "pink") +
  stat_regline_equation(label.x = 12, label.y = .75, aes(label = ..eq.label..), geom = "label", color = r2_color, size = 4, fontface = "bold") +
  geom_hline(yintercept = gt_intercept, linetype = "dashed", color = gt_color, size = .8) +
  scale_x_continuous(limits = c(0,75),
                     breaks = c(0, 20, 40, 60)) +
  scale_y_continuous(limits = c(0,1),
                     breaks = c(0, .2, .4, .6, .8, 1)) +
  labs(title = "b) Random forest\n") +
  theme_minimal() +
  theme(text = element_text(face = "bold"),
        plot.title = element_text(face = "bold"),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12)
        )

# Collate with patchwork
patch <- p18 + p19
patch + plot_annotation(subtitle = "\nClassifiers trained on Old Books dataset (9 classes). Each point represents a noise/engine version (n=132) of the corpus.\nX axis cropped.\n\n",
                        caption = "\nWord error (percent)",
                        theme = theme(plot.subtitle = element_text(size = 12),
                                      plot.caption = element_text(hjust = 0.5, size = 12, margin = margin(t = 0), face = "bold")))
```

## Topic modelling

Assessing the effect of OCR error on topic models is more complicated, since they involve more judgment calls and do not yield an obvious indicator of accuracy. I used the `stm` R package to fit structural topic models to all the versions of the corpus. As a first step, I ran the `stm::searchK()` function for a *k* value range from 6 to 20, on the suspicion that different variants of the text might yield different diagnostics and hence inspire different choices for the number of topics in the model. Figure 9a shows that the *k* intercept for the high point of the held-out likelihood curve varies from 6 to 12 depending on the version of the corpus. Held-out likelihood is not the only criterion for selecting *k*, but it is an important one, so these results suggests that even a small amount of OCR error can lead researchers to choose a different topic number than they would have done on a cleaner text, with concomitant effects on the substantive analysis. Moreover, if we hold *k* still at 8 --- the value suggested by diagnostics of the ground truth version of the corpus --- we see in Figure 9b that the semantic coherence of the model decreases slightly with more noise. 

```{r echo=FALSE, fig.cap='OCR error and topic model fits', fig.height=6, fig.width=10, message=FALSE, warning=FALSE}
#============
# Figure 9
#============

# Load data
topmod <- read.csv(here("data", "topic_models_master.csv"))
topmod_collapsed <- topmod %>% 
  group_by(uniq, engine) %>% 
  summarize(mean_word_error = mean(mean_word_error), highest_heldout_k = mean(highest_heldout_k))

## A) k values
gt_intercept <- topmod %>% select(highest_heldout_k) %>% .[1,]
highlight_df <- topmod_collapsed %>% filter(engine == "ground truth")

p20 <- ggplot(topmod_collapsed, aes(x = mean_word_error, y = highest_heldout_k)) +
  geom_point(color = points_color, size = 4, alpha = .5) +
  geom_point(data = highlight_df, color = gt_color, size = 4) +
  geom_hline(yintercept = gt_intercept, linetype = "dashed", color = gt_color, size = .8) +
  annotate("text", x = 10, y = 8.5, label = "Ground truth", color = gt_color, fontface = "bold", size = 4) +
  scale_x_continuous(limits = c(0,75),
                     breaks = c(0, 20, 40, 60)) +
  scale_y_continuous(limits = c(5,12),
                     breaks = c(5, 6, 7, 8, 9, 10, 11, 12)) +
  labs(title = "b) Suggested k values",
       subtitle = "(k intercepts for held-out likelihood high points.)\n",
       y = "k") +
  theme_minimal() +
  theme(text = element_text(face = "bold"),
    plot.title = element_text(face = "bold"),
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12))

# B) Semantic coherence
topmod_8k <- topmod %>% filter(K==8)
gt_intercept <- topmod %>% filter(K==8) %>% filter(engine=="ground truth") %>% select(semcoh) %>% .[1,]
highlight_df <- topmod_8k %>% filter(engine == "ground truth")

p21 <- ggplot(topmod_8k, aes(x = mean_word_error, y = semcoh)) +
  geom_point(color = points_color, size = 4, alpha = .5) +
  geom_point(data = highlight_df, color = gt_color, size = 4) +
  geom_hline(yintercept = gt_intercept, linetype = "dashed", color = gt_color, size = .8) +
  scale_x_continuous(limits = c(0,75),
                     breaks = c(0, 20, 40, 60)) +
  scale_y_continuous(limits = c(-90, -10),
                     breaks = c(-80, -60, -40, -20, 0)) +
  labs(title = "b) Semantic coherence (k=8)\n",
       y = "Semantic coherence") +
  theme_minimal() +
  theme(text = element_text(face = "bold"),
    plot.title = element_text(face = "bold"),
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12)
    )

# Collate with Patchwork
patch <- p20 + p21
patch + plot_annotation(subtitle = "Structural topic models of Old Books dataset, built with R package stm. Each point represents a noise/engine version (n=132)\nof the corpus. X axes cropped.\n\n",
                        caption = "\nWord error (percent)",
                        theme = theme(plot.subtitle = element_text(size = 12),
                                      plot.caption = element_text(hjust = 0.5, size = 12, margin = margin(t = 0), face = "bold")))
```

## Named entity recognition

Our corpus is full of names and dates, so a researcher might also want to explore it with named entity recognition (NER) models. I used a pretrained `spaCy` model (`en_core_web_sm`) to extract entities from all non-preprocessed versions of the corpus and compared the output to that of the ground truth text. In the absence of ground truth NER label data, I treated `spaCy`'s prediction for the ground truth text as the reference point and calculated the F1 score (the harmonic average of precision and recall) as a metric for accuracy. For simplicity, the evaluation included only predicted entity names, not entity labels. Figure 10 shows that OCR error affected NER accuracy severely. In a real-life setting these effects would be partly mitigated by pre- and postprocessing, but it seems reasonable to suggest that NER is one of the areas where the value added from high-precision OCR is the highest.

```{r echo=FALSE, fig.cap='OCR error and named entity recognition accuracy', fig.height=6, fig.width=10, out.width = "50%", fig.align='center', message=FALSE, warning=FALSE}
#============
# Figure 10
#============

highlight_df <- clf %>% filter(engine == "ground truth")

p22 <- ggplot(clf, aes(x = mean_word_error, y = ner_f1, )) +
  geom_point(color = points_color, size = 4, alpha = .5) +
  geom_point(data = highlight_df, color = gt_color, size = 4) +
  geom_smooth(method="lm", color="pink", fill="pink") +
  stat_regline_equation(label.x = 40, label.y = .5, aes(label = ..eq.label..), geom = "label", color = r2_color, size = 4, fontface = "bold") +
  annotate("text", x = 13, y = 1, label = "Ground truth", color = gt_color, fontface="bold", size = 4) +
  scale_x_continuous(limits = c(0,75),
                     breaks = c(0, 20, 40, 60)) +
  scale_y_continuous(limits = c(0,1),
                     breaks = c(0, .2, .4, .6, .8, 1)
                     ) +
  labs(subtitle = "Named entities extracted from Old Books dataset with spaCy model\nand compared with output from ground truth text. Each point represents\na noise/engine version (n=132) of the corpus. X axes cropped.\n\n",
       x="\nWord error (percent)",
       y="F1 score") +
  theme_minimal() +
  theme(text = element_text(face = "bold"),
        plot.subtitle = element_text(size = 11, face = "plain"),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12)
        )

p22 + plot_layout(widths = c(1, 1))
```

Broadly speaking, these tests indicate that OCR error mattered the most in NER, the least in topic modelling and sentiment analysis, while in classification there was a tipping point at around 20 percent OCR error. At the same time, all the tests showed some accuracy deterioration even at very low OCR error rates.  

# Conclusion

This article described a systematic test of three general OCR processors on a large new dataset of English and Arabic documents. It suggests that the server-based engines Document AI and Textract deliver markedly higher out-of-the-box accuracy than the standalone Tesseract library, especially on noisy documents. It also indicates that certain types of "integrated" noise, such as blur and salt and pepper, generate more error than "superimposed" noise such as watermarks, scribbles, and even ink stains. Furthermore, it suggests that the "OCR language gap" still persists, although Document AI seems to have partially closed it, at least for Arabic. 

The key takeaway for the social sciences and humanities is that high-accuracy OCR is now more accessible than ever before. Researchers who might be deterred by the prospect of extensive document preprocessing or corpus-specific model training now have at their disposal user-friendly tools that deliver strong results out of the box. This will likely lead to more scholars adopting OCR technology and to more historical documents becoming digitized. 

The findings can also help scholars tailor OCR solutions to their needs. For many users and use cases, server-based OCR processing will be an efficient option. However, there are are downsides to consider, such as processing fees and data privacy concerns, which means that in some cases, other solutions --- such as self-trained Tesseract models or even plain Tesseract --- might be preferable.[^privacy] Having baseline data on relative processor performance and differential effects of noise types can help navigate such tradeoffs and optimise one's workflow.

[^privacy]: Amazon openly says it "may store and use document and image inputs [...] to improve and develop the quality of Amazon Textract and other Amazon machine-learning/artificial-intelligence technologies" (see https://aws.amazon.com/textract/faqs/, accessed 3 September 2021). Google says it "does not use any of your content [...] for any purpose except to provide you with the Document AI API service" (see https://cloud.google.com/document-ai/docs/data-usage, accessed 3 September 2021), but it is unclear what lies in the word "provide" and whether it includes the training of the processor.

The study has several limitations, notably that it tested only three processors on two languages with a non-exhaustive list of noise types. This means we cannot say which processor is the very best on the market or provide a comprehensive guide to OCR performance on all languages and noise types. However, the test design used here can easily be applied to other processors, languages, and noise types for a more complete picture. Another limitation is that the experiment only used single-column test materials, which does not capture layout parsing capabilities. Most OCR engines, including Document AI and Textract, still struggle with multi-column text, and even state-of-the-art tools such as Layout Parser [@shen2021layoutparser] require corpus-specific training for accurate results. Future studies will need to determine which processors deliver the best out-of-the-box layout parsing. In any case, we appear to be in the middle of a small revolution in OCR technology with potentially large benefits for the social sciences and humanities.

\newpage

# Conflicts of interest

On behalf of all authors, the corresponding author states that there is no conflict of interest.

# References






